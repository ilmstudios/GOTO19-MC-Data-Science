{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision Trees\n",
    "\n",
    "Reference: [An Introduction to Statistical Learning](http://www-bcf.usc.edu/~gareth/ISL/) (This is one of the must-read books for data scientists)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Decision trees** are _non-parametric_ models that can either be used for regression or classification. They are the building blocks for **random forest** and **xgboost** models, which are often used to win machine learning competitions. You are likely very familiar with decision trees already! For example, a rule-based engine in your organization.\n",
    "\n",
    "**Random forests** are groups of decision trees created using different subsets and feature sets of the training data. Each tree \"votes\" on a classification or \"predicts\" on a regression task. We will cover it later."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Why are we learning about decision trees?\n",
    "\n",
    "- They can be applied to both regression and classification problems.\n",
    "- They are easy to explain to others (interpretable).\n",
    "- They are very popular among data scientists.\n",
    "- They are the basis for more sophisticated models.\n",
    "- They have a different way of \"thinking\" than the other models we have studied."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regression Trees - Numeric Output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For instance, suppose we wanted to predict Major League Baseball salaries from 1986â€“87 using the following data set.\n",
    "\n",
    "- **Years** (x-axis): Number of years playing in the major leagues.\n",
    "- **Hits** (y-axis): Number of hits in the previous year.\n",
    "- **Salary** (color): Low salary is blue/green, high salary is red/yellow.\n",
    "\n",
    "![Salary data](./img/salary_color.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you plotted these data in three dimensions, with salary corresponding to height, *linear regression* would find the plane that goes through the heart of the data (minimizes the sum of squared distances from the data points)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A **regression tree** takes a different approach. It breaks the 2D plane shown into segments according to the following rules:\n",
    "\n",
    "- You can only use **straight lines** that are drawn one at a time.\n",
    "- Your line must either be **vertical or horizontal**.\n",
    "- Your line **stops** when it hits an existing line.\n",
    "\n",
    "It then predicts the average value within a segment for every point in that segment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For instance, it might start with a vertical line that separates most of the purple points from the rest:\n",
    "\n",
    "![](./img/salary_color_split1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next it might split between veterans with high and low numbers of hits:\n",
    "\n",
    "![](./img/salary_color_split2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In general, instead of a plane, a decision tree regression model gives you \"stair steps.\"\n",
    "\n",
    "![](./img/tree_3d_plot.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is another way to visualize the model:\n",
    "\n",
    "![Salary tree annotated](./img/salary_tree_annotated.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first split is **years < 4.5**, thus that split goes at the top of the tree. When a splitting rule is **true**, you follow the left branch. When a splitting rule is **false**, you follow the right branch.\n",
    "\n",
    "For players in the **left branch**, the mean salary is \\$166,000, thus you label it with that value. (Salary has been divided by 1,000 and log-transformed to 5.11.)\n",
    "\n",
    "For players in the **right branch**, there is a further split on **hits < 117.5**, dividing players into two more salary regions: \\$403,000 (transformed to 6.00), and \\$846,000 (transformed to 6.74)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:** Years and hits are both integers, but the convention is to use the **midpoint** between adjacent values to label a split."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What does this tree tell you about your data?**\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building Decision Trees for Classification Tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's get a new dataset again\n",
    "!mkdir ./data/breast_cancer_wisconsin\n",
    "!curl https://archive.ics.uci.edu/ml/machine-learning-databases/breast-cancer-wisconsin/breast-cancer-wisconsin.data > ./data/breast_cancer_wisconsin/breast-cancer-wisconsin.data\n",
    "!curl https://archive.ics.uci.edu/ml/machine-learning-databases/breast-cancer-wisconsin/breast-cancer-wisconsin.names > ./data/breast_cancer_wisconsin/breast-cancer-wisconsin.names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cat ./data/breast_cancer_wisconsin/breast-cancer-wisconsin.names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Requirements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fpath = './data/breast_cancer_wisconsin/breast-cancer-wisconsin.data'\n",
    "colnames = ['id', 'clump_thickness', 'uniformity_of_cell_size', 'uniformity_of_cell_shape',\n",
    "            'marginal_adhesion', 'single_epithelial_cell_size', 'bare_nuclei',\n",
    "            'bland_chromatin', 'normal_nucleoli', 'mitoses', 'target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(fpath, names=colnames)\n",
    "\n",
    "# transform a string value in `bare_nuclei` into unknown\n",
    "df['bare_nuclei'] = df.bare_nuclei.apply(lambda x: 0 if not x.isnumeric() else int(x))\n",
    "df.dropna(inplace=True)\n",
    "df.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Explore the data by sorting, plotting, or performing split-apply-combine (a.k.a. `group_by`). Decide which feature you think is the most important predictor, and use it to create a splitting rule. (Only binary splits are allowed.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = df.loc[:, 'target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for var in colnames[1:-1]:\n",
    "    print(var)\n",
    "    x = df.loc[:, var]\n",
    "    plt.scatter(x, y)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What do you think?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Surprisingly, this type of non-linear, meaning non-linearly-separable, problems are perfect for tree based methods, for which we will begin learning from decision tree algorithms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How Does a Computer Build a Regression Tree?\n",
    "\n",
    "**Ideal approach:** Consider every possible partition of the feature space.\n",
    "\n",
    "**Problem:** Too many possibilities to consider.\n",
    "\n",
    "**\"Good enough\" approach:** Recursive binary splitting.\n",
    "\n",
    "1. Begin at the top of the tree.\n",
    "2. For **every feature**, examine **every possible cutpoint**, and choose the feature and cutpoint so that the resulting tree has the lowest possible error. Make that split.\n",
    "3. Examine the two resulting regions. Once again, make a **single split** (in one of the regions) to minimize the MSE.\n",
    "4. Keep repeating Step 3 until a **stopping criterion** is met:\n",
    "    - Maximum tree depth (maximum number of splits required to arrive at a leaf).\n",
    "    - Minimum number of observations in a leaf.\n",
    "\n",
    "---\n",
    "\n",
    "This approach is a **greedy algorithm** because it makes *locally optimal* decisions -- it takes the best split at each step. Greedy algorithms typically are not optimal, but they are often good enough and relatively easy to compute.\n",
    "\n",
    "**Analogy:**\n",
    "- Always eating cookies to maximize your immediate happiness (greedy) might not lead to optimal overall happiness.\n",
    "- In our case, reorganizing parts of the tree already constructed based on future splits might result in a better model overall. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building a Classification Tree in `scikit-learn`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "treeclf = DecisionTreeClassifier(random_state=123)\n",
    "treeclf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**what does our target variable look like?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.target.value_counts() # 2 for benign, 4 for malignant"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's first transform the label into 0 and 1, representing benign and malignant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['target'] = df.target.map({2:0, 4:1})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.target.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define X and y.\n",
    "feature_cols = colnames[1:-1]\n",
    "X = df.loc[:, feature_cols]\n",
    "y = df.loc[:, 'target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use leave-one-out cross-validation (LOOCV) to estimate the precision for this model.\n",
    "# we will discuss how to train models properly later, where we will touch on LOOCV\n",
    "from sklearn.model_selection import cross_val_score\n",
    "scores = cross_val_score(treeclf, X, y, scoring='precision')\n",
    "np.mean(scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Precision` evaluates how accurate the predicted positive cases are. For example, if a model predicts 10 instances to be positive, and 9 of them are actually positive, the precision is 0.9."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In sum, the process of builiding a decision tree for classification task in Scikit-Learn is very similar to that to build linear regression and logistic regression. We go through three steps:\n",
    "\n",
    "    1. Define predictors (X) and predicted variable or target(Y)\n",
    "    2. Instantiate the model\n",
    "    3. \"Fit\" the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building a Regression Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's get a new dataset again\n",
    "!curl https://archive.ics.uci.edu/ml/machine-learning-databases/00332/OnlineNewsPopularity.zip > ./data/OnlineNewsPopularity.zip\n",
    "!unzip ./data/OnlineNewsPopularity.zip -d ./data/\n",
    "!rm ./data/OnlineNewsPopularity.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are the path to the data:\n",
    "\n",
    "    Data path: ./data/OnlineNewsPopularity/OnlineNewsPopularity.csv\n",
    "    Meta Data Path: ./data/OnlineNewsPopularity/OnlineNewsPopularity.names  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cat ./data/OnlineNewsPopularity/OnlineNewsPopularity.names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise**\n",
    "\n",
    "Try use the above example to build a regression tree for this dataset.\n",
    "\n",
    "More about the dataset: [documentation](https://archive.ics.uci.edu/ml/datasets/Online+News+Popularity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fpath = './data/OnlineNewsPopularity/OnlineNewsPopularity.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(fpath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparing Regression Trees and Classification Trees\n",
    "\n",
    "|Regression Trees|Classification Trees|\n",
    "|---|---|\n",
    "|Predict a continuous response.|Predict a categorical response.|\n",
    "|Predict using mean response of each leaf.|Predict using most commonly occurring class of each leaf.|\n",
    "|Splits are chosen to minimize MSE.|Splits are chosen to minimize Gini index (discussed below).|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Splitting Criteria for Classification Trees\n",
    "\n",
    "Common options for the splitting criteria:\n",
    "\n",
    "- **Classification error rate:** The fraction of training observations in a region that don't belong to the most common class.\n",
    "- **Gini index:** The measure of total variance across classes in a region."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example: Classification Error Rate\n",
    "\n",
    "Pretend we are predicting whether or not someone will buy an iPhone or an Android:\n",
    "\n",
    "- At a particular node, there are **25 observations** (phone buyers) of whom **10 bought iPhones and 15 bought Androids**.\n",
    "- As the majority class is **Android**, that's our prediction for all 25 observations, and thus the classification error rate is **10/25 = 40%**.\n",
    "\n",
    "Our goal in making splits is to **reduce the classification error rate**. Let's try splitting on gender:\n",
    "\n",
    "- **Males:** 2 iPhones and 12 Androids, thus the predicted class is Android.\n",
    "- **Females:** 8 iPhones and 3 Androids, thus the predicted class is iPhone.\n",
    "- Classification error rate after this split would be **5/25 = 20%**.\n",
    "\n",
    "Compare that with a split on age:\n",
    "\n",
    "- **30 or younger:** 4 iPhones and 8 Androids, thus the predicted class is Android.\n",
    "- **31 or older:** 6 iPhones and 7 Androids, thus the predicted class is Android.\n",
    "- Classification error rate after this split would be **10/25 = 40%**.\n",
    "\n",
    "The decision tree algorithm will try **every possible split across all features** and choose the one that **reduces the error rate the most.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example: Gini Index\n",
    "\n",
    "Calculate the Gini index before making a split:\n",
    "\n",
    "$$1 - \\left(\\frac {iPhone} {Total}\\right)^2 - \\left(\\frac {Android} {Total}\\right)^2 = 1 - \\left(\\frac {10} {25}\\right)^2 - \\left(\\frac {15} {25}\\right)^2 = 0.48$$\n",
    "\n",
    "- The **maximum value** of the Gini index is 0.5 and occurs when the classes are perfectly balanced in a node.\n",
    "- The **minimum value** of the Gini index is 0 and occurs when there is only one class represented in a node.\n",
    "- A node with a lower Gini index is said to be more \"pure.\"\n",
    "\n",
    "Evaluating the split on **gender** using the Gini index:\n",
    "\n",
    "$$\\text{Males: } 1 - \\left(\\frac {2} {14}\\right)^2 - \\left(\\frac {12} {14}\\right)^2 = 0.24$$\n",
    "$$\\text{Females: } 1 - \\left(\\frac {8} {11}\\right)^2 - \\left(\\frac {3} {11}\\right)^2 = 0.40$$\n",
    "$$\\text{Weighted Average: } 0.24 \\left(\\frac {14} {25}\\right) + 0.40 \\left(\\frac {11} {25}\\right) = 0.31$$\n",
    "\n",
    "Evaluating the split on **age** using the Gini index:\n",
    "\n",
    "$$\\text{30 or younger: } 1 - \\left(\\frac {4} {12}\\right)^2 - \\left(\\frac {8} {12}\\right)^2 = 0.44$$\n",
    "$$\\text{31 or older: } 1 - \\left(\\frac {6} {13}\\right)^2 - \\left(\\frac {7} {13}\\right)^2 = 0.50$$\n",
    "$$\\text{Weighted Average: } 0.44 \\left(\\frac {12} {25}\\right) + 0.50 \\left(\\frac {13} {25}\\right) = 0.47$$\n",
    "\n",
    "Again, the decision tree algorithm will try **every possible split** and will choose the one that **reduces the Gini index (and thus increases the \"node purity\") the most**.\n",
    "\n",
    "You can think of this as each split increasing the accuracy of predictions. If there is some error at a node, then splitting at that node will result in two nodes with a higher average \"node purity\" than the original. So, we ensure continually better fits to the training data by continually splitting nodes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification Error Rate vs. Gini Index\n",
    "\n",
    "- Gini index is the default in sklearn.\n",
    "- One advantage Gini index is that it will sometimes make splits that do not improve accuracy but do give better predicted probabilities."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary: Comparing Decision Trees With Other Models\n",
    "\n",
    "**Advantages of decision trees:**\n",
    "\n",
    "- They can be used for regression or classification.\n",
    "- They can be displayed graphically.\n",
    "- They are highly interpretable.\n",
    "- They can be specified as a series of rules, and more closely approximate human decision-making than other models.\n",
    "- Prediction is fast.\n",
    "- Their features don't need scaling.\n",
    "- They automatically learn feature interactions.\n",
    "- Tends to ignore irrelevant features.\n",
    "- They are non-parametric -- as a result, they will outperform linear models if the relationship between features and response is highly non-linear."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BONUS\n",
    "Other splitting criteria to look into (for classification trees)\n",
    "- [entropy](https://bricaud.github.io/personal-blog/entropy-in-decision-trees/)\n",
    "- [information gain ratio](https://www3.nd.edu/~rjohns15/cse40647.sp14/www/content/lectures/23%20-%20Decision%20Trees%202.pdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
